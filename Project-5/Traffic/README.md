### My experimentation process

I start with layers as the same as lecture video using Conv2D with filters = 32, kernel_size = (3,3) and input_shape = (IMG_WIDTH, IMG_HEIGHT, 3). For pooling, I use max pooling 2D (same as lecture video) with size (3, 3), after that is Flatten() and finally the output layers with NUM_CATEGORIES nodes and activation is softmax. Starting by changing filter by from 32 drop to 31, 30 and increase to 33, 34 and with that changes, accuracy all cases drop from 0.9404 to 0.9169, 0.9054 and 0.9126, 0.9084 with higher loss than case when filter equals to 32. I also change size of kernel and pool size by increasing and decreasing by 1 but the results are not better than original. (the result might be difference from each training)

After changing some properties of layers in model, I decide to add 1 hidden layers with range from 120 to 150 units (reference from lecture) the accuracy starts to increase. After increase to 256 they start to fail so I set units of hidden layers to 200. When using dropout, the accuracy drops significantly from 0.9xxx to 0.6xxx (even I start to decrease % of dropout) with this information I discard dropout layer from network.